{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import time \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 5\n",
    "class_size = 10\n",
    "lr = 0.001\n",
    "\n",
    "input_dim = 784\n",
    "hidden_dim = 256\n",
    "latent_dim = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "make_dir(\"./output/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output_image(image, name):\n",
    "    img = image.view(image.size(0), 1, 28, 28)\n",
    "    save_image(img, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(idx, n=class_size):\n",
    "\n",
    "    assert idx.shape[1] == 1\n",
    "    assert torch.max(idx).item() < n\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx.data, 1)\n",
    "\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data/mnist\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Variational AutoEncoder (CVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim , class_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim+class_size, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.linear(x))\n",
    "        mean = self.mu(hidden)\n",
    "        log_var = self.var(hidden)\n",
    "        \n",
    "        return mean, log_var\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, class_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(latent_dim+class_size, hidden_dim)\n",
    "        self.hidden_to_out = nn.Linear(hidden_dim, output_dim)\n",
    "          \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.latent_to_hidden(x))\n",
    "        x = torch.sigmoid(self.hidden_to_out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAVE\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, class_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim, class_size)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim, class_size)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((x,y), dim=1)\n",
    "        z_mu, z_var = self.encoder(x)\n",
    "        \n",
    "        # Reparameterize\n",
    "        std = torch.exp(z_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps*std + z_mu\n",
    "        \n",
    "        z = torch.cat((x_sample, y), dim=1)\n",
    "        \n",
    "        generated_x = self.decoder(z)\n",
    "        \n",
    "        return generated_x, z_mu, z_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(input_dim, hidden_dim, latent_dim, class_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CVAE(\n",
       "  (encoder): Encoder(\n",
       "    (linear): Linear(in_features=794, out_features=256, bias=True)\n",
       "    (mu): Linear(in_features=256, out_features=75, bias=True)\n",
       "    (var): Linear(in_features=256, out_features=75, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (latent_to_hidden): Linear(in_features=85, out_features=256, bias=True)\n",
       "    (hidden_to_out): Linear(in_features=256, out_features=784, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, mean, log_var):\n",
    "    # reconstruction loss\n",
    "    RCL = F.binary_cross_entropy(reconstructed_x, x,)\n",
    "    # kl divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return RCL + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = x.to(get_device())\n",
    "\n",
    "        y = onehot(y.view(-1, 1))\n",
    "        y = y.to(get_device())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstructed_x, z_mu, z_var = model(x, y)\n",
    "\n",
    "        loss = calculate_loss(x, reconstructed_x, z_mu, z_var)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloader_test):\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = x.to(get_device())\n",
    "\n",
    "            y = onehot(y.view(-1, 1))\n",
    "            y = y.to(get_device())\n",
    "\n",
    "            reconstructed_x, z_mu, z_var = model(x, y)\n",
    "\n",
    "            loss = calculate_loss(x, reconstructed_x, z_mu, z_var)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.3547, Test Loss: 0.2240\n",
      "Epoch 1, Train Loss: 0.2239, Test Loss: 0.2214\n",
      "Epoch 2, Train Loss: 0.2227, Test Loss: 0.2209\n",
      "Epoch 3, Train Loss: 0.2224, Test Loss: 0.2207\n",
      "Epoch 4, Train Loss: 0.2221, Test Loss: 0.2204\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "for e in range(epochs):\n",
    "\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "\n",
    "    train_loss /= len(dataloader_train)\n",
    "    test_loss /= len(dataloader_test)\n",
    "\n",
    "    print(f'Epoch {e}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 1\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a random latent vector\n",
    "z = torch.randn(1, latent_dim).to(get_device())\n",
    "\n",
    "# pick randomly 1 class, for which we want to generate the data\n",
    "y = torch.randint(0, class_size, (1, 1)).to(dtype=torch.long)\n",
    "print(f'Generating a {y.item()}')\n",
    "\n",
    "y = onehot(y).to(get_device(), dtype=z.dtype)\n",
    "z = torch.cat((z, y), dim=1)\n",
    "\n",
    "reconstructed_img = model.decoder(z)\n",
    "img = reconstructed_img.view(28, 28).data\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
